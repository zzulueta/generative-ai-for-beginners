{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Install the Required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install semantic-kernel==0.9.1b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the environment variables file\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import openai\n",
    "\n",
    "# Environment variable obtained from Azure Cosmos DB for MongoDB vCore\n",
    "AZCOSMOS_CONNSTR=os.getenv(\"AZCOSMOS_CONNSTR\")\n",
    "AZCOSMOS_API=os.getenv(\"AZCOSMOS_API\")\n",
    "AZCOSMOS_DATABASE_NAME=os.getenv(\"AZCOSMOS_DATABASE_NAME\")\n",
    "AZCOSMOS_CONTAINER_NAME=os.getenv(\"AZCOSMOS_CONTAINER_NAME\")\n",
    "\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_key = os.getenv(\"AZURE_OPENAI_KEY\") \n",
    "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\") \n",
    "openai.api_deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "openai.api_embeddings_deployment_name = os.getenv(\"AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT\")\n",
    "openai.api_version = \"2023-07-01-preview\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the parameters needed by [Azure Cosmos DB for MongoDB vCore](https://learn.microsoft.com/azure/cosmos-db/mongodb/vcore/vector-search) to create the vector search index are handled by semantic kernel.\n",
    "\n",
    "In this guide, we are using `text-embedding-ada-002` embedding model to generate the embeddings which uses a 1536-dimensional embedding vector.\n",
    "\n",
    "The `num_lists` is an integer that represents of clusters that the inverted file (IVF) index uses to group the vector data.\n",
    "\n",
    "The `similarity` used with IVF index here is the `COS` (cosine distance) but you can also try `L2` (Euclidean distance), and `IP` (inner product). For more information see the [Understand embeddings in Azure OpenAI Service article](https://learn.microsoft.com/azure/ai-services/openai/concepts/understand-embeddings#cosine-similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# collection name will be used multiple times in the code so we store it in a variable\n",
    "collection_name = AZCOSMOS_CONTAINER_NAME\n",
    "\n",
    "# Vector search index parameters\n",
    "index_name = \"VectorSearchIndex\"\n",
    "vector_dimensions = (\n",
    "    1536  # text-embedding-ada-002 uses a 1536-dimensional embedding vector\n",
    ")\n",
    "num_lists = 1\n",
    "similarity = \"COS\"  # cosine distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes in a json file of NoSQL records and checks if your data exists in the database using the id of the record then skips the record if it exists or generates embeddings and uploads the database record along with it's embedding.\n",
    "\n",
    "The `save_information` function does two things: generate embeddings + upload the data to your database.\n",
    "\n",
    "Learn more about the semantic kernel memory store [here](https://learn.microsoft.com/semantic-kernel/memories/) and the embeddings [here](https://learn.microsoft.com/semantic-kernel/memories/embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from semantic_kernel.memory.semantic_text_memory import SemanticTextMemory\n",
    "from semantic_kernel.memory.memory_store_base import MemoryStoreBase\n",
    "\n",
    "\n",
    "async def upsert_data_to_memory_store(\n",
    "    memory: SemanticTextMemory, store: MemoryStoreBase, data_file_path: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    This asynchronous function takes two memory stores and a data file path as arguments.\n",
    "    It is designed to upsert (update or insert) data into the memory stores from the data file.\n",
    "\n",
    "    Args:\n",
    "        kernel_memory_store (callable): A callable object that represents the kernel memory store where data will be upserted.\n",
    "        memory_store (callable): A callable object that represents the memory store where data will be upserted.\n",
    "        data_file_path (str): The path to the data file that contains the data to be upserted.\n",
    "\n",
    "    Returns:\n",
    "        None. The function performs an operation that modifies the memory stores in-place.\n",
    "    \"\"\"\n",
    "    with open(file=data_file_path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        n = 0\n",
    "        for item in data:\n",
    "            n += 1\n",
    "            # check if the item already exists in the memory store\n",
    "            # if the id doesn't exist, it throws an exception\n",
    "            try:\n",
    "                already_created = bool(\n",
    "                    await store.get(\n",
    "                        collection_name, item[\"id\"], with_embedding=True\n",
    "                    )\n",
    "                )\n",
    "            except Exception:\n",
    "                already_created = False\n",
    "            # if the record doesn't exist, we generate embeddings and save it to the database\n",
    "            if not already_created:\n",
    "                await memory.save_information(\n",
    "                    collection=collection_name,\n",
    "                    id=item[\"id\"],\n",
    "                    # the embedding is generated from the text field\n",
    "                    text=item[\"content\"],\n",
    "                    description=item[\"title\"],\n",
    "                )\n",
    "                print(\n",
    "                    \"Generating embeddings and saving new item:\",\n",
    "                    n,\n",
    "                    \"/\",\n",
    "                    len(data),\n",
    "                    end=\"\\r\",\n",
    "                )\n",
    "            else:\n",
    "                print(\"Skipping item already exits:\", n, \"/\", len(data), end=\"\\r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Add the Chat and Embedding models to the Semantic Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the semantic kernel, and initialize the semantic kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantic_kernel as sk\n",
    "\n",
    "# Intialize the kernel\n",
    "kernel = sk.Kernel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the needed libraries.\n",
    "\n",
    "We need the chat completion for having a conversation and text embeddings for generating embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.ai.open_ai import (\n",
    "    AzureChatCompletion,\n",
    "    AzureTextEmbedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the chat deployment name, initialize the chat completions with the required parameters, and add the created chat service to the semantic kernel instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added Azure OpenAI Chat Service...\n"
     ]
    }
   ],
   "source": [
    "# adding azure openai chat service\n",
    "chat_model_deployment_name = os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "endpoint = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_key = os.environ.get(\"AZURE_OPENAI_KEY\")\n",
    "\n",
    "kernel.add_service(\n",
    "    AzureChatCompletion(\n",
    "        service_id=\"chat_completion\",\n",
    "        deployment_name=chat_model_deployment_name,\n",
    "        endpoint=endpoint,\n",
    "        api_key=api_key,\n",
    "    )\n",
    ")\n",
    "print(\"Added Azure OpenAI Chat Service...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the embeddings deployment name and initialize the text embedding with the required parameters, and add the created embedding service to the semantic kernel instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added Azure OpenAI Embedding Generation Service...\n"
     ]
    }
   ],
   "source": [
    "# adding azure openai text embedding service\n",
    "embedding_model_deployment_name = os.environ.get(\n",
    "    \"AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT\"\n",
    ")\n",
    "\n",
    "kernel.add_service(\n",
    "    AzureTextEmbedding(\n",
    "        service_id=\"text_embedding\",\n",
    "        deployment_name=embedding_model_deployment_name,\n",
    "        endpoint=endpoint,\n",
    "        api_key=api_key,\n",
    "    )\n",
    ")\n",
    "print(\"Added Azure OpenAI Embedding Generation Service...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Create or Update Azure Cosmos DB for MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The semantic kernel can handle the database, collection, index creation.\n",
    "\n",
    "Import the Azure CosmosDB memory store and initialize it with the parameters defined before.\n",
    "\n",
    "If the database, collection, and index exist it won't overwrite it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating or updating Azure Cosmos DB Memory Store...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/semantic_kernel/connectors/memory/azure_cosmosdb/cosmosdb_utils.py:32: UserWarning: You appear to be connected to a CosmosDB cluster. For more information regarding feature compatibility and support please visit https://www.mongodb.com/supportability/cosmosdb\n",
      "  return MongoClient(cosmos_conn_str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished updating Azure Cosmos DB Memory Store...\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel.connectors.memory.azure_cosmosdb import (\n",
    "    AzureCosmosDBMemoryStore,\n",
    ")\n",
    "\n",
    "print(\"Creating or updating Azure Cosmos DB Memory Store...\")\n",
    "# create azure cosmos db for mongo db vcore api store and collection with vector ivf\n",
    "# currently, semantic kernel only supports the ivf vector kind\n",
    "store = await AzureCosmosDBMemoryStore.create(\n",
    "    cosmos_connstr=AZCOSMOS_CONNSTR,\n",
    "    cosmos_api=AZCOSMOS_API,\n",
    "    database_name=AZCOSMOS_DATABASE_NAME,\n",
    "    collection_name=AZCOSMOS_CONTAINER_NAME,\n",
    "    index_name=index_name,\n",
    "    vector_dimensions=vector_dimensions,\n",
    "    num_lists=num_lists,\n",
    "    similarity=similarity,\n",
    ")\n",
    "print(\"Finished updating Azure Cosmos DB Memory Store...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record with ID 10: <semantic_kernel.memory.memory_record.MemoryRecord object at 0x7fe69836dbb0>\n",
      "['sampleCollection']\n"
     ]
    }
   ],
   "source": [
    "# Query a specific record by its ID\n",
    "record_id = \"10\"\n",
    "record = await store.get(AZCOSMOS_CONTAINER_NAME, record_id, with_embedding=True)\n",
    "    \n",
    "# Print the retrieved record\n",
    "if record:\n",
    "    print(f\"Record with ID {record_id}: {record}\")\n",
    "else:\n",
    "    print(f\"No record found with ID {record_id}\")\n",
    "\n",
    "sample = await store.get_collections()\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the created memory store to the semantic kernel instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered Azure Cosmos DB Memory Store...\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel.memory.semantic_text_memory import SemanticTextMemory\n",
    "from semantic_kernel.core_plugins.text_memory_plugin import TextMemoryPlugin\n",
    "\n",
    "memory = SemanticTextMemory(storage=store, embeddings_generator=kernel.get_service(\"text_embedding\"))\n",
    "kernel.import_plugin_from_object(TextMemoryPlugin(memory), \"TextMemoryPluginACDB\")\n",
    "print(\"Registered Azure Cosmos DB Memory Store...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Generate embeddings and Create Database records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the helper function with the JSON data file to generate embeddings and create or update the database records.\n",
    "\n",
    "If the records already exit it will skip it.\n",
    "\n",
    "Records are identified by their ids.\n",
    "\n",
    "The data used here is a dummy data which you can replace with your own.\n",
    "\n",
    "**Note that you need to specify id, text, and description fields.\n",
    "The text field is what gets converted to embeddings.**\n",
    "\n",
    "See the helper function definition for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Upserting data to Azure Cosmos DB Memory Store...\")\n",
    "await upsert_data_to_memory_store(memory, store, \"./data/text-sample.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Test the Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The search function converts the query_term to a vector embedding and finds the similarity between it and the database records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each time it calls the embedding model to generate embeddings from your query\n",
    "query_term = \"What is Azure Logic apps?\"\n",
    "result = await memory.search(collection_name, query_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result is: Azure Logic Apps is a cloud-based service that allows you to create and run workflows that integrate with various services and data sources. It provides a visual designer for creating workflows, and supports a wide range of connectors for popular services like Office 365, Salesforce, and Dropbox. Logic Apps enable you to automate tasks, process and transform data, and orchestrate complex processes without writing code. You can monitor and manage your workflows using Azure Portal, REST APIs, or PowerShell.\n",
      "Relevance Score: 0.9261224357936126\n",
      "Full Record: {\"text\": \"Azure Logic Apps is a cloud-based service that allows you to create and run workflows that integrate with various services and data sources. It provides a visual designer for creating workflows, and supports a wide range of connectors for popular services like Office 365, Salesforce, and Dropbox. Logic Apps enable you to automate tasks, process and transform data, and orchestrate complex processes without writing code. You can monitor and manage your workflows using Azure Portal, REST APIs, or PowerShell.\", \"description\": \"Azure Logic Apps\", \"additional_metadata\": null}\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Result is: {result[0].text}\\nRelevance Score: {result[0].relevance}\\nFull Record: {result[0].additional_metadata}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Create chat function with Azure OpenAI chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "    You are a chatbot that can have a conversations about any topic related to the provided context.\n",
    "    Give explicit answers from the provided context or say 'I don't know' if it does not have an answer.\n",
    "    provided context: {{$db_record}}\n",
    "\n",
    "    User: {{$query_term}}\n",
    "    Chatbot:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantic_kernel.connectors.ai.open_ai as sk_oai\n",
    "\n",
    "execution_settings = sk_oai.OpenAITextPromptExecutionSettings(\n",
    "   service_id=\"chat_completion\",\n",
    "    ai_model_id=chat_model_deployment_name,\n",
    "    max_tokens=500,\n",
    "    temperature=0.0,\n",
    "    top_p=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.prompt_template.input_variable import InputVariable\n",
    "\n",
    "chat_prompt_template_config = sk.PromptTemplateConfig(\n",
    "    template=prompt,\n",
    "    name=\"grounded_response\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    input_variables=[\n",
    "        InputVariable(name=\"db_record\", description=\"The database record\", is_required=True),\n",
    "        InputVariable(name=\"query_term\", description=\"The user input\", is_required=True),\n",
    "    ],\n",
    "    execution_settings=execution_settings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_function = kernel.create_function_from_prompt(\n",
    "    prompt=prompt,\n",
    " function_name= \"ChatGPTFunc2\", plugin_name=\"chatGPTPlugin2\", prompt_template_config=chat_prompt_template_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "completions_result = await kernel.invoke(chat_function, sk.KernelArguments(query_term=query_term, db_record=result[0].additional_metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure Logic Apps is a cloud-based service that allows you to create and run workflows that integrate with various services and data sources. It provides a visual designer for creating workflows and supports a wide range of connectors for popular services like Office 365, Salesforce, and Dropbox. Azure Logic Apps enable you to automate tasks, process and transform data, and orchestrate complex processes without writing code. You can monitor and manage your workflows using Azure Portal, REST APIs, or PowerShell.\n"
     ]
    }
   ],
   "source": [
    "print(completions_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Testing the RAG flow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "what are azure functions?\n",
      "Response:\n",
      "Azure Functions is a serverless compute service that allows you to run event-driven code without the need to manage the underlying infrastructure. It offers features such as automatic scaling, triggers, and bindings. Azure Functions supports various programming languages like C#, Java, and Python. It can be used to build microservices, integrate with other Azure services, and process and transform data. It also integrates with other Azure services like Azure Event Hubs, Azure Storage, and Azure Cosmos DB.\n",
      "\n",
      "Question:\n",
      "how about azure machine learning\n",
      "Response:\n",
      "Azure Machine Learning is a fully managed, end-to-end platform that allows you to build, train, and deploy machine learning models at scale. It offers features like automated machine learning, data labeling, and model management. It supports various frameworks, including TensorFlow, PyTorch, and Scikit-learn. Azure Machine Learning can be used to develop predictive analytics solutions, improve decision-making, and gain insights into your data. It also integrates with other Azure services like Azure Databricks and Azure Synapse Analytics.\n",
      "\n"
     ]
    },
    {
     "ename": "ServiceResponseException",
     "evalue": "(\"<class 'semantic_kernel.connectors.ai.open_ai.services.azure_text_embedding.AzureTextEmbedding'> service failed to generate embeddings\", BadRequestError('Error code: 400 - {\\'error\\': {\\'message\\': \"\\'$.input\\' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", \\'type\\': \\'invalid_request_error\\', \\'param\\': None, \\'code\\': None}}'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/semantic_kernel/connectors/ai/open_ai/services/open_ai_handler.py:80\u001b[0m, in \u001b[0;36mOpenAIHandler._send_embedding_request\u001b[0;34m(self, settings)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msettings\u001b[38;5;241m.\u001b[39mprepare_settings_dict())\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore_usage(response)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/openai/resources/embeddings.py:215\u001b[0m, in \u001b[0;36mAsyncEmbeddings.create\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m--> 215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    217\u001b[0m     body\u001b[38;5;241m=\u001b[39mmaybe_transform(params, embedding_create_params\u001b[38;5;241m.\u001b[39mEmbeddingCreateParams),\n\u001b[1;32m    218\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    219\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers,\n\u001b[1;32m    220\u001b[0m         extra_query\u001b[38;5;241m=\u001b[39mextra_query,\n\u001b[1;32m    221\u001b[0m         extra_body\u001b[38;5;241m=\u001b[39mextra_body,\n\u001b[1;32m    222\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    223\u001b[0m         post_parser\u001b[38;5;241m=\u001b[39mparser,\n\u001b[1;32m    224\u001b[0m     ),\n\u001b[1;32m    225\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mCreateEmbeddingResponse,\n\u001b[1;32m    226\u001b[0m )\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/openai/_base_client.py:1816\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1813\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1814\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1815\u001b[0m )\n\u001b[0;32m-> 1816\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/openai/_base_client.py:1510\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m   1502\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1503\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     remaining_retries: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1509\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[0;32m-> 1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1511\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1512\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1513\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1514\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1515\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m   1516\u001b[0m     )\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/openai/_base_client.py:1611\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1610\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1611\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1613\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1614\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1615\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1619\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39mget_max_retries(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries) \u001b[38;5;241m-\u001b[39m retries,\n\u001b[1;32m   1620\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mServiceResponseException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m query_term \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      5\u001b[0m     query_term \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter a query: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m memory\u001b[38;5;241m.\u001b[39msearch(collection_name, query_term)\n\u001b[1;32m      7\u001b[0m     completions_result \u001b[38;5;241m=\u001b[39m kernel\u001b[38;5;241m.\u001b[39minvoke_stream(chat_function, sk\u001b[38;5;241m.\u001b[39mKernelArguments(query_term\u001b[38;5;241m=\u001b[39mquery_term, db_record\u001b[38;5;241m=\u001b[39mresult[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39madditional_metadata))\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mquery_term\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mResponse:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/semantic_kernel/memory/semantic_text_memory.py:142\u001b[0m, in \u001b[0;36mSemanticTextMemory.search\u001b[0;34m(self, collection, query, limit, min_relevance_score, with_embeddings)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch\u001b[39m(\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    124\u001b[0m     collection: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m     with_embeddings: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    129\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[MemoryQueryResult]:\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search the memory (calls the memory store's get_nearest_matches method).\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m    Arguments:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m        List[MemoryQueryResult] -- The list of MemoryQueryResult found.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     query_embedding \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embeddings_generator\u001b[38;5;241m.\u001b[39mgenerate_embeddings([query]))[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    143\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage\u001b[38;5;241m.\u001b[39mget_nearest_matches(\n\u001b[1;32m    144\u001b[0m         collection_name\u001b[38;5;241m=\u001b[39mcollection,\n\u001b[1;32m    145\u001b[0m         embedding\u001b[38;5;241m=\u001b[39mquery_embedding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    148\u001b[0m         with_embeddings\u001b[38;5;241m=\u001b[39mwith_embeddings,\n\u001b[1;32m    149\u001b[0m     )\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [MemoryQueryResult\u001b[38;5;241m.\u001b[39mfrom_memory_record(r[\u001b[38;5;241m0\u001b[39m], r[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results]\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/semantic_kernel/connectors/ai/open_ai/services/open_ai_text_embedding_base.py:38\u001b[0m, in \u001b[0;36mOpenAITextEmbeddingBase.generate_embeddings\u001b[0;34m(self, texts, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m     batch \u001b[38;5;241m=\u001b[39m texts[i : i \u001b[38;5;241m+\u001b[39m batch_size]  \u001b[38;5;66;03m# noqa: E203\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     settings\u001b[38;5;241m.\u001b[39minput \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m---> 38\u001b[0m     raw_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_embedding_request(\n\u001b[1;32m     39\u001b[0m         settings\u001b[38;5;241m=\u001b[39msettings,\n\u001b[1;32m     40\u001b[0m     )\n\u001b[1;32m     41\u001b[0m     raw_embeddings\u001b[38;5;241m.\u001b[39mextend(raw_embedding)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m array(raw_embeddings)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/semantic_kernel/connectors/ai/open_ai/services/open_ai_handler.py:86\u001b[0m, in \u001b[0;36mOpenAIHandler._send_embedding_request\u001b[0;34m(self, settings)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [array(x\u001b[38;5;241m.\u001b[39membedding) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mdata]\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceResponseException(\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m service failed to generate embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     88\u001b[0m         ex,\n\u001b[1;32m     89\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mex\u001b[39;00m\n",
      "\u001b[0;31mServiceResponseException\u001b[0m: (\"<class 'semantic_kernel.connectors.ai.open_ai.services.azure_text_embedding.AzureTextEmbedding'> service failed to generate embeddings\", BadRequestError('Error code: 400 - {\\'error\\': {\\'message\\': \"\\'$.input\\' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", \\'type\\': \\'invalid_request_error\\', \\'param\\': None, \\'code\\': None}}'))"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "query_term = \"\"\n",
    "while query_term != \"exit\":\n",
    "    query_term = input(\"Enter a query: \")\n",
    "    result = await memory.search(collection_name, query_term)\n",
    "    completions_result = kernel.invoke_stream(chat_function, sk.KernelArguments(query_term=query_term, db_record=result[0].additional_metadata))\n",
    "    print(f\"Question:\\n{query_term}\\nResponse:\")\n",
    "    async for completion in completions_result:\n",
    "        print(str(completion[0]), end=\"\")\n",
    "    print(\"\\n\")\n",
    "    time.sleep(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
